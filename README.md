<div align="center">

# MolecularIQ Benchmark Dataset

**A comprehensive benchmark for evaluating large language models on molecular reasoning tasks**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![RDKit](https://img.shields.io/badge/Built%20with-RDKit-3838ff.svg)](https://www.rdkit.org/)

<p align="center">
  <em>Count, index, and constraint generation questions across diverse chemical features</em>
</p>

![MolecularIQ benchmark statistics](assets/moleculariq_statistics.png)

</div>

---

## ğŸ¯ Overview

**MolecularIQ** is a benchmark specifically designed to measure the **structural reasoning abilities** of large language models on molecules. Unlike many chemistry evaluation sets that rely on literature labels or surrogate predictors, MolecularIQ focuses only on tasks whose correctness can be **verified algorithmically** from the molecular graph itself. This makes it possible to distinguish genuine structural understanding from memorization or correlation-based answers.


## ğŸ“ Repository Structure

```
moleculariq-benchmark/
â”œâ”€â”€ ğŸ“‚ src/                                # Source code
â”‚   â”œâ”€â”€ ğŸ“‚ a_dataset_pools/               # Stage A: Dataset pool creation
â”‚   â”‚   â”œâ”€â”€ 1_collect_pubchem_data.py
â”‚   â”‚   â”œâ”€â”€ 2_collect_external_test_set_molecules.py
â”‚   â”‚   â”œâ”€â”€ 3_standardize_pubchem_mols_and_remove_external_test_mols.py
â”‚   â”‚   â”œâ”€â”€ 4_create_train_test_pools.py
â”‚   â”‚   â”œâ”€â”€ 5_create_hard_test_pool_dataframe.py
â”‚   â”‚   â””â”€â”€ utils/                        # External test set utilities
â”‚   â””â”€â”€ ğŸ“‚ b_benchmark/                   # Stage B: Benchmark generation
â”‚       â”œâ”€â”€ 1_compute_properties.py       # Compute ground truth properties
â”‚       â”œâ”€â”€ 2_create_benchmark.py         # Generate final benchmark dataset
â”‚       â”œâ”€â”€ task_names.py                 # Task name definitions
â”‚       â””â”€â”€ benchmark_generator/          # Generation logic (uses moleculariq-core)
â”‚           â”œâ”€â”€ main.py                   # CLI entry point
â”‚           â”œâ”€â”€ config.py                 # Configuration
â”‚           â”œâ”€â”€ tasks/                    # Task generators (count, index, constraint)
â”‚           â”œâ”€â”€ core/                     # Sampling, scoring, validation
â”‚           â””â”€â”€ output/                   # JSON & HuggingFace export
â”œâ”€â”€ ğŸ“‚ data/                              # Data artifacts (not tracked)
â”‚   â”œâ”€â”€ dataset_pools/                    # Molecule pools
â”‚   â”‚   â”œâ”€â”€ external/                     # External benchmark molecules
â”‚   â”‚   â”œâ”€â”€ intermediate/                 # Pipeline intermediates
â”‚   â”‚   â”œâ”€â”€ processed/                    # Processed datasets
â”‚   â”‚   â”œâ”€â”€ pseudo_sdf/                   # Sample SDF for testing
â”‚   â”‚   â””â”€â”€ pubchem_raw_sdf/              # Raw PubChem SDF files
â”‚   â””â”€â”€ benchmark/                        # Generated benchmark data
â”‚       â”œâ”€â”€ properties.pkl                # Precomputed molecular properties
â”‚       â””â”€â”€ benchmark_dataset.json        # Final benchmark dataset
â”œâ”€â”€ ğŸ““ notebooks/                         # Analysis notebooks
â”‚   â””â”€â”€ overview_created_data.ipynb       # Data creation walkthrough
â””â”€â”€ ğŸ“Š assets/                            # Documentation assets
    â””â”€â”€ moleculariq_statistics.png
```

## ğŸ”„ Data Creation Pipeline

### Stage A: Dataset Pool Creation

**1. Collect PubChem Data** â†’ [`1_collect_pubchem_data.py`](src/a_dataset_pools/1_collect_pubchem_data.py)
   - Extract SMILES and IUPAC names from PubChem SDF files
   - Filter molecules (carbon-containing, single-fragment)

**2. Collect External Test Sets** â†’ [`2_collect_external_test_set_molecules.py`](src/a_dataset_pools/2_collect_external_test_set_molecules.py)
   - Aggregate molecules from LLaSMol, ChemDFM, Ether0, ChemIQ benchmarks

**3. Standardize and Filter** â†’ [`3_standardize_pubchem_mols_and_remove_external_test_mols.py`](src/a_dataset_pools/3_standardize_pubchem_mols_and_remove_external_test_mols.py)
   - Canonicalize SMILES
   - Remove molecules present in external benchmarks

**4. Create Train/Test Pools** â†’ [`4_create_train_test_pools.py`](src/a_dataset_pools/4_create_train_test_pools.py)
   - Cluster molecules using MinHash LSH on Morgan fingerprints
   - Split into: Training pool, Easy test set, Hard test set

**5. Create Hard Test Pool DataFrame** â†’ [`5_create_hard_test_pool_dataframe.py`](src/a_dataset_pools/5_create_hard_test_pool_dataframe.py)
   - Build structured dataframe with molecular complexity metrics

### Stage B: Benchmark Generation

**1. Compute Properties** â†’ [`1_compute_properties.py`](src/b_benchmark/1_compute_properties.py)
   - Calculate ground truth values for all molecular properties
   - Uses `SymbolicSolver` from [moleculariq-core](https://github.com/ml-jku/moleculariq-core) for accurate computation

**2. Create Benchmark** â†’ [`2_create_benchmark.py`](src/b_benchmark/2_create_benchmark.py)
   - Sample diverse datapoints across complexity dimensions
   - Generate questions using natural language templates
   - Create single/multi count, index, and constraint generation tasks
   - Export to JSON and HuggingFace dataset formats


## ğŸš€ Getting Started

### Prerequisites

```bash
# Install moleculariq-core
pip install git+https://github.com/ml-jku/moleculariq-core.git

# Then install this package
pip install .  
# or pip install -e . for development
```

### Quick Start

#### 1. Download PubChem SDF files (optional - pseudo SDF included for testing)

```bash
# Download from https://pubchem.ncbi.nlm.nih.gov/docs/downloads
# Place in data/dataset_pools/pubchem_raw_sdf/
```

#### 2. Run the data creation pipeline

```bash
# Stage A: Create molecule pools (run from repo root)
python src/a_dataset_pools/1_collect_pubchem_data.py
python src/a_dataset_pools/2_collect_external_test_set_molecules.py
python src/a_dataset_pools/3_standardize_pubchem_mols_and_remove_external_test_mols.py
python src/a_dataset_pools/4_create_train_test_pools.py
python src/a_dataset_pools/5_create_hard_test_pool_dataframe.py

# Stage B: Generate benchmark (run from repo root)
python src/b_benchmark/1_compute_properties.py
python src/b_benchmark/2_create_benchmark.py
```

#### 3. Explore the created data

```bash
jupyter notebook notebooks/overview_created_data.ipynb
```

## ğŸ‘¨â€ğŸ‘§â€ğŸ‘¦ MolecularIQ Family

This package is part of the MolecularIQ ecosystem:

| Repository | Purpose | 
|------------|---------|
| **[moleculariq](https://anonymous.4open.science/r/moleculariq-EE40/README.md)** | Central hub for the MolecularIQ benchmark ecosystem|
| **moleculariq-leaderboard** | Leaderboard: HuggingFace space, displays results, handles submissions; please see supplementary material |
|  **[moleculariq-core](https://anonymous.4open.science/r/moleculariq-core-2F02/README.md)** | MolecularIQD and shared library providing core functionality, e.g. symbolic verifiers and question formatting | 
| ğŸ“ **[moleculariq-benchmark](https://anonymous.4open.science/r/moleculariq-benchmark-5A40/README.md)** | Dataset creation: task definitions, symbolic verifiers implementations, question generator| 
| **moleculariq-eval** | Evaluation code: integration with lm-eval-harness, model configs, reward functions, extraction functions, and system prompts; please see supplementary material|

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---
